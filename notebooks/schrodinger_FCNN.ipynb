{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schrodinger FCNN PyTorch\n",
    "\n",
    "This notebook contains the code originally from the Python script `schrodinger_FCNN_pytorch.py`.\n",
    "\n",
    "**Project Goal:** Use a Fully Connected Neural Network (FCNN) in PyTorch to predict the ground state wavefunction of the 1D Schrodinger equation.\n",
    "\n",
    "**Important:**\n",
    "1.  Ensure you have placed the necessary data file (e.g., `NN_valid_data_schrodinger_1D_sol-Rx=3-Nx=128-num_data=25000-size_data=129.csv`) in the `../data/` directory relative to this notebook.\n",
    "2.  Create an `../outputs/` directory relative to this notebook. The training results (model files, plots) will be saved there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import csv\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchrodingerFCNN(nn.Module): # Renamed from MLP\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a FCNN for predicting Schrodinger equation wavefunctions.\n",
    "    Uses nn.Linear layers and Softplus activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(SchrodingerFCNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "        # Dynamically create layers based on hidden_sizes list\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Note: Bias is True by default in nn.Linear\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            # Add activation except for the last layer\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                self.layers.append(nn.Softplus()) # Using Softplus as in the original code\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchrodingerFCNNTrainer: # Renamed from MLPTrainer\n",
    "    \"\"\"\n",
    "    Handles data loading, training (manual batching), evaluation, and plotting\n",
    "    for the SchrodingerFCNN model. Assumes notebook is in 'notebooks/' and data/outputs are one level up.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 # Data parameters\n",
    "                 Rx=3, Nx=128,\n",
    "                 all_data=8000,\n",
    "                 data_path_prefix='../data', # Relative path from notebooks/\n",
    "                 output_path_prefix='../outputs', # Relative path from notebooks/\n",
    "                 data_file_name_template='NN_valid_data_schrodinger_1D_sol-Rx={Rx}-Nx={Nx}-num_data={num_data}-size_data={size_data}.csv',\n",
    "                 num_data_in_filename=25000,\n",
    "                 # Model architecture parameters\n",
    "                 hidden_sizes=[128],\n",
    "                 # Training parameters\n",
    "                 batch_size=200,\n",
    "                 learning_rate=0.01,\n",
    "                 epochnum=200,\n",
    "                 weight_decay=1e-4,\n",
    "                 shuffle_each_epoch=True, # Added shuffle option\n",
    "                 early_stopping_patience=10): # Added patience\n",
    "\n",
    "        self.Rx = Rx\n",
    "        self.Nx = Nx\n",
    "        self.data_size = Nx - 1         # Input size (potential grid points)\n",
    "        self.output_neuron = Nx - 1     # Output size (wavefunction grid points)\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.all_data_to_use = all_data # Use only this many samples\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochnum = epochnum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.data_path_prefix = data_path_prefix\n",
    "        self.output_path_prefix = output_path_prefix # Store output path\n",
    "        self.shuffle_each_epoch = shuffle_each_epoch # Store shuffle preference\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(self.output_path_prefix, exist_ok=True)\n",
    "        print(f\"Outputs will be saved to: {os.path.abspath(self.output_path_prefix)}\")\n",
    "\n",
    "        # Construct filename\n",
    "        size_data_in_filename = self.Nx + 1 # This seems to be the convention in the filename\n",
    "        self.data_file_name = data_file_name_template.format(\n",
    "            Rx=self.Rx, Nx=self.Nx, num_data=num_data_in_filename, size_data=size_data_in_filename\n",
    "        )\n",
    "        self.data_full_path = os.path.join(self.data_path_prefix, self.data_file_name) # Use prefix\n",
    "        print(f\"Attempting to load data from: {os.path.abspath(self.data_full_path)}\")\n",
    "\n",
    "\n",
    "        self.dx = 2 * self.Rx / self.Nx\n",
    "        # Define x points for plotting\n",
    "        self.x = np.linspace(-self.Rx + self.dx/2, self.Rx - self.dx/2, self.Nx - 1)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self._load_data() # Load data during initialization\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = SchrodingerFCNN(\n",
    "            input_size=self.data_size,\n",
    "            hidden_sizes=self.hidden_sizes,\n",
    "            output_size=self.output_neuron\n",
    "        ).to(self.device)\n",
    "\n",
    "        print(\"\\nModel Architecture:\")\n",
    "        print(self.model)\n",
    "\n",
    "        # Optimizer and Loss\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        self.criterion = nn.MSELoss(reduction='mean') # Use mean reduction for consistent scaling\n",
    "\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Loads data, processes, splits into tensors (no DataLoaders).\"\"\"\n",
    "        print(f\"Loading data from: {self.data_full_path}\")\n",
    "\n",
    "        try:\n",
    "            with open(self.data_full_path, newline='') as csvfile:\n",
    "                dataconfig = list(csv.reader(csvfile))\n",
    "                dataconfig = np.array(dataconfig, dtype=\"float32\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Data file not found at {self.data_full_path}\")\n",
    "            print(f\"Please ensure the data file exists in the '{self.data_path_prefix}' directory relative to where you run this notebook.\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or parsing data: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Determine column indices based on Nx\n",
    "        # Assuming format: E_0, V(x_1)...V(x_{Nx-1}), psi_0, psi(x_1)...psi(x_{Nx-1})\n",
    "        # Total columns = 1 (E_0) + (Nx-1) (V) + 1 (psi_0) + (Nx-1) (psi) = 2*Nx\n",
    "        # Potential: Column 1 to Nx-1 (0-based index: 1 to Nx-1) -> Nx-1 columns\n",
    "        # Wave func: Column Nx+1 to 2*Nx-1 (0-based index: Nx+1 to 2*Nx-1) -> Nx-1 columns\n",
    "        potential_start_col = 1\n",
    "        potential_end_col = self.Nx # Exclusive index\n",
    "        fnum_start_col = self.Nx + 1 # +1 because psi_0 is at Nx\n",
    "        fnum_end_col = 2 * self.Nx   # Exclusive index\n",
    "\n",
    "        potential_raw = dataconfig[:self.all_data_to_use, potential_start_col:potential_end_col]\n",
    "        fnum_raw = dataconfig[:self.all_data_to_use, fnum_start_col:fnum_end_col]\n",
    "\n",
    "        # --- Shape Validation ---\n",
    "        if potential_raw.shape[1] != self.data_size:\n",
    "             raise ValueError(f\"Potential data width ({potential_raw.shape[1]}) != expected input size ({self.data_size})\")\n",
    "        if fnum_raw.shape[1] != self.output_neuron:\n",
    "             raise ValueError(f\"Wavefunction data width ({fnum_raw.shape[1]}) != expected output size ({self.output_neuron})\")\n",
    "        if potential_raw.shape[0] < self.all_data_to_use:\n",
    "            print(f\"Warning: Loaded fewer samples ({potential_raw.shape[0]}) than requested ({self.all_data_to_use}). Using available samples.\")\n",
    "            self.all_data_to_use = potential_raw.shape[0]\n",
    "            if self.all_data_to_use == 0:\n",
    "                 raise ValueError(\"No data samples loaded.\")\n",
    "        # --- End Shape Validation ---\n",
    "\n",
    "\n",
    "        # Convert to tensors\n",
    "        potential_all = torch.from_numpy(potential_raw).float()\n",
    "        fnum_all = torch.from_numpy(fnum_raw).float()\n",
    "\n",
    "        # Split data: 60% train, 20% validation, 20% test\n",
    "        total_samples = potential_all.shape[0]\n",
    "        self.training_data_count = int(total_samples * 0.60)\n",
    "        self.validation_data_count = int(total_samples * 0.20)\n",
    "        self.test_data_count = total_samples - self.training_data_count - self.validation_data_count\n",
    "\n",
    "        # Ensure counts add up and adjust test count if necessary due to rounding\n",
    "        if self.training_data_count + self.validation_data_count + self.test_data_count != total_samples:\n",
    "             self.test_data_count = total_samples - self.training_data_count - self.validation_data_count\n",
    "\n",
    "        train_end_idx = self.training_data_count\n",
    "        val_end_idx = self.training_data_count + self.validation_data_count\n",
    "\n",
    "        # Slice tensors\n",
    "        self.potential_train = potential_all[:train_end_idx]\n",
    "        self.fnum_train = fnum_all[:train_end_idx]\n",
    "        self.potential_validation = potential_all[train_end_idx:val_end_idx]\n",
    "        self.fnum_validation = fnum_all[train_end_idx:val_end_idx]\n",
    "        self.potential_test = potential_all[val_end_idx:]\n",
    "        self.fnum_test = fnum_all[val_end_idx:]\n",
    "\n",
    "        # Update counts just in case slicing resulted in different sizes (shouldn't happen)\n",
    "        self.training_data_count = self.potential_train.shape[0]\n",
    "        self.validation_data_count = self.potential_validation.shape[0]\n",
    "        self.test_data_count = self.potential_test.shape[0]\n",
    "\n",
    "        print(f\"Data loaded successfully:\")\n",
    "        print(f\"  Training samples:   {self.training_data_count}\")\n",
    "        print(f\"  Validation samples: {self.validation_data_count}\")\n",
    "        print(f\"  Testing samples:    {self.test_data_count}\")\n",
    "\n",
    "\n",
    "    def calculate_loss(self, f_pred, f_target):\n",
    "        \"\"\"Calculates the scaled loss.\"\"\"\n",
    "        # Raw MSE Loss\n",
    "        mse_loss = self.criterion(f_pred, f_target)\n",
    "        # Scale the loss as done in the original script\n",
    "        # loss = dx * sum( (pred - target)^2 )\n",
    "        # mse_loss = sum( (pred - target)^2 ) / N\n",
    "        # => sum( (pred - target)^2 ) = mse_loss * N\n",
    "        # => scaled_loss = dx * mse_loss * N\n",
    "        scaled_loss = self.dx * mse_loss * self.output_neuron # N is output_neuron size\n",
    "        return scaled_loss\n",
    "\n",
    "    def train_one_epoch(self, current_epoch_indices):\n",
    "        \"\"\"Performs a single training epoch using manual batch slicing.\"\"\"\n",
    "        self.model.train() # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        samples_processed = 0\n",
    "        num_batches = (self.training_data_count + self.batch_size - 1) // self.batch_size # Calculate total batches\n",
    "\n",
    "        # Iterate through training data using indices for this epoch\n",
    "        for i in range(0, self.training_data_count, self.batch_size):\n",
    "            batch_indices = current_epoch_indices[i:min(i + self.batch_size, self.training_data_count)]\n",
    "            if len(batch_indices) == 0: continue # Skip if indices are empty\n",
    "\n",
    "            # Get batch data using indices\n",
    "            batch_potential = self.potential_train[batch_indices].to(self.device)\n",
    "            batch_fnum = self.fnum_train[batch_indices].to(self.device)\n",
    "\n",
    "            current_batch_size = len(batch_indices)\n",
    "            samples_processed += current_batch_size\n",
    "\n",
    "            # Forward pass\n",
    "            f_pred = self.model(batch_potential)\n",
    "            loss = self.calculate_loss(f_pred, batch_fnum)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Accumulate loss (weighted by batch size)\n",
    "            running_loss += loss.item() * current_batch_size\n",
    "\n",
    "            # Optional: Print batch progress (commented out for cleaner notebook)\n",
    "            # current_batch_num = (i // self.batch_size) + 1\n",
    "            # print(f'  Batch {current_batch_num}/{num_batches} - Loss: {loss.item():.3f}')\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        avg_epoch_loss = running_loss / samples_processed if samples_processed > 0 else 0\n",
    "        return avg_epoch_loss, samples_processed\n",
    "\n",
    "\n",
    "    def evaluate(self, potential_set, fnum_set, data_count):\n",
    "        \"\"\"Evaluates the model on a given data set (test or validation).\"\"\"\n",
    "        self.model.eval() # Set model to evaluation mode\n",
    "        total_loss = 0.0\n",
    "        num_samples = 0\n",
    "        if data_count == 0:\n",
    "             print(\"Warning: Attempting to evaluate on an empty dataset.\")\n",
    "             return float('inf') # Or 0, depending on desired behavior\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient calculations\n",
    "            for i in range(0, data_count, self.batch_size):\n",
    "                end_idx = min(i + self.batch_size, data_count)\n",
    "                if i >= end_idx: continue # Should not happen with correct logic\n",
    "\n",
    "                # Get batch data\n",
    "                batch_potential = potential_set[i:end_idx].to(self.device)\n",
    "                batch_fnum = fnum_set[i:end_idx].to(self.device)\n",
    "\n",
    "                current_batch_size = len(batch_potential)\n",
    "                if current_batch_size == 0: continue\n",
    "\n",
    "                # Forward pass\n",
    "                f_pred = self.model(batch_potential)\n",
    "                loss = self.calculate_loss(f_pred, batch_fnum)\n",
    "\n",
    "                # Accumulate loss\n",
    "                total_loss += loss.item() * current_batch_size\n",
    "                num_samples += current_batch_size\n",
    "\n",
    "        # Calculate average loss\n",
    "        avg_loss_per_sample = total_loss / num_samples if num_samples > 0 else float('inf')\n",
    "        self.model.train() # Set model back to training mode\n",
    "        return avg_loss_per_sample\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Main training loop over multiple epochs.\"\"\"\n",
    "        print(\"\\n--- Starting Training ---\")\n",
    "        epoch_train_losses = []\n",
    "        epoch_valid_losses = []\n",
    "        best_valid_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = None\n",
    "        early_stop_triggered = False\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        indices = torch.arange(self.training_data_count) # Base indices\n",
    "\n",
    "        for epoch in range(self.epochnum):\n",
    "            # Shuffle indices for the current epoch if enabled\n",
    "            if self.shuffle_each_epoch:\n",
    "                epoch_indices = indices[torch.randperm(self.training_data_count)]\n",
    "            else:\n",
    "                epoch_indices = indices\n",
    "\n",
    "            # Train one epoch\n",
    "            avg_train_loss, samples_in_epoch = self.train_one_epoch(epoch_indices)\n",
    "            epoch_train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            avg_valid_loss = self.evaluate(self.potential_validation, self.fnum_validation, self.validation_data_count)\n",
    "            epoch_valid_losses.append(avg_valid_loss)\n",
    "            print(f\"Epoch {epoch+1}/{self.epochnum} complete. \"\n",
    "                  f\"Avg Training Loss: {avg_train_loss:.3f} | \"\n",
    "                  f\"Avg Validation Loss: {avg_valid_loss:.3f}\")\n",
    "\n",
    "            # Early Stopping Check\n",
    "            if avg_valid_loss < best_valid_loss:\n",
    "                best_valid_loss = avg_valid_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = self.model.state_dict() # Save the best state\n",
    "                # print(f\"  (New best validation loss: {best_valid_loss:.3f}. Saving model state)\") # Verbose\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                # print(f\"  (Validation loss did not improve for {epochs_no_improve} epoch(s))\") # Verbose\n",
    "                if epochs_no_improve >= self.early_stopping_patience:\n",
    "                    print(f\"\\n\\nEarly stopping triggered after {epoch+1} epochs.\")\n",
    "                    early_stop_triggered = True\n",
    "                    break # Exit training loop\n",
    "\n",
    "        stop_time = timeit.default_timer()\n",
    "        total_time = stop_time - start_time\n",
    "\n",
    "        # Determine which model state to use and save\n",
    "        save_suffix = \"\"\n",
    "        if early_stop_triggered and best_model_state is not None:\n",
    "             print(\"\\nLoading best model state from early stopping.\")\n",
    "             self.model.load_state_dict(best_model_state)\n",
    "             save_suffix = '_best'\n",
    "        else:\n",
    "             # If no early stopping or best state wasn't saved (shouldn't happen), save final state\n",
    "             print(\"\\nUsing final model state from last epoch.\")\n",
    "             save_suffix = '_final'\n",
    "\n",
    "\n",
    "        print(f\"\\n--- Training Finished ---\")\n",
    "        print(f\"Total Training time: {total_time:.2f} seconds\")\n",
    "\n",
    "        # Plot losses\n",
    "        self._plot_loss(epoch_train_losses, epoch_valid_losses)\n",
    "\n",
    "        # Final evaluation on test set using the chosen model state\n",
    "        final_test_loss = self.evaluate(self.potential_test, self.fnum_test, self.test_data_count)\n",
    "        print(f\"\\nFinal Test Loss (using {'best' if early_stop_triggered and best_model_state is not None else 'final'} model): {final_test_loss:.3f}\")\n",
    "\n",
    "        # Plot some predictions\n",
    "        self._plot_predictions(num_plots=5)\n",
    "\n",
    "        # Save the chosen model state\n",
    "        self.save_model(suffix=save_suffix)\n",
    "\n",
    "\n",
    "    def _plot_loss(self, epoch_train_losses, epoch_valid_losses):\n",
    "        \"\"\"Plots the average training and validation loss per epoch.\"\"\"\n",
    "        print(\"\\n--- Plotting Training and Validation Loss ---\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        num_epochs_recorded = len(epoch_train_losses)\n",
    "        epochs_range = range(1, num_epochs_recorded + 1)\n",
    "        plt.plot(epochs_range, epoch_train_losses, marker='o', linestyle='-', label='Training Loss')\n",
    "        plt.plot(epochs_range, epoch_valid_losses, marker='x', linestyle='--', label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Average Loss')\n",
    "        plt.yscale('log') # Use log scale for better visualization if losses vary widely\n",
    "        plt.grid(True, which=\"both\", ls=\"--\", alpha=0.6)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(self.output_path_prefix, 'fcnn_train_validation_loss.png') # Use prefix\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved loss plot to {save_path}\")\n",
    "        plt.show() # Display plot in notebook\n",
    "        plt.close() # Close the figure\n",
    "\n",
    "\n",
    "    def _plot_predictions(self, num_plots=5):\n",
    "        \"\"\"Plots model predictions against target values for a few test samples.\"\"\"\n",
    "        print(f\"\\n--- Plotting {num_plots} Test Predictions ---\")\n",
    "        self.model.eval() # Ensure model is in eval mode\n",
    "\n",
    "        if self.test_data_count < num_plots:\n",
    "            print(f\"Warning: Requested {num_plots} plots, but only {self.test_data_count} test samples available. Plotting all.\")\n",
    "            num_plots = self.test_data_count\n",
    "        if num_plots <= 0:\n",
    "             print(\"No test samples to plot.\")\n",
    "             return\n",
    "\n",
    "        # Select indices evenly spaced across the test set\n",
    "        plot_indices = np.linspace(0, self.test_data_count - 1, num_plots, dtype=int)\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient calculations for plotting\n",
    "            for i, sample_idx_in_test_set in enumerate(plot_indices):\n",
    "                # Get data for the specific sample\n",
    "                potential_sample_tensor = self.potential_test[sample_idx_in_test_set].unsqueeze(0).to(self.device) # Add batch dim\n",
    "                fnum_target_numpy = self.fnum_test[sample_idx_in_test_set].cpu().numpy()\n",
    "                potential_input_numpy = self.potential_test[sample_idx_in_test_set].cpu().numpy()\n",
    "\n",
    "                # Make prediction\n",
    "                f_pred_tensor = self.model(potential_sample_tensor)\n",
    "                f_pred_numpy = f_pred_tensor.squeeze(0).cpu().numpy() # Remove batch dim\n",
    "\n",
    "                # Calculate loss for this specific sample\n",
    "                loss_sample = self.calculate_loss(f_pred_tensor,\n",
    "                                                  self.fnum_test[sample_idx_in_test_set].unsqueeze(0).to(self.device)).item()\n",
    "\n",
    "                # --- Plotting ---\n",
    "                fig, ax1 = plt.subplots(dpi=120) # Create figure and primary axis for wavefunction\n",
    "\n",
    "                # Plot wavefunction (Predicted vs Target)\n",
    "                ax1.plot(self.x, f_pred_numpy, color='blue', linewidth=2, label=r'Predicted $\\\\psi(x)$')\n",
    "                ax1.plot(self.x, fnum_target_numpy, color='red', linestyle='--', label=r'Target $\\\\psi(x)$')\n",
    "                ax1.set_xlabel(r'$x$')\n",
    "                ax1.set_ylabel(r'$\\psi(x)$', color='black')\n",
    "                ax1.tick_params(axis='y', labelcolor='black')\n",
    "                ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "                # Create secondary axis for potential\n",
                    "                ax2 = ax1.twinx()\\n",
                "                ax2.plot(self.x, potential_input_numpy, color='green', linestyle=':', alpha=0.7, label='Potential V(x)')\\n",
                "                ax2.set_ylabel('Potential V(x)', color='green')\\n",
                "                ax2.tick_params(axis='y', labelcolor='green')\\n",
                "\\n",
                "                # Combine legends\\n",
                "                lines1, labels1 = ax1.get_legend_handles_labels()\\n",
                "                lines2, labels2 = ax2.get_legend_handles_labels()\\n",
                "                ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\\n",
                "\\n",
                "                # Calculate original index in the full dataset\\n",
                "                full_data_index = self.training_data_count + self.validation_data_count + sample_idx_in_test_set\\n",
                "                ax1.set_title(f'FCNN Test Sample (Full Idx: {full_data_index}) | Loss: {loss_sample:.3f}')\\n",
                "\\n",
                "                # Save and show plot\\n",
                "                plot_filename = os.path.join(self.output_path_prefix, f'fcnn_prediction_sample_{full_data_index}.png') # Use prefix\\n",
                "                plt.savefig(plot_filename)\\n",
                "                print(f\\\"Saved prediction plot to {plot_filename}\\\")\\n",
                "                plt.show() # Display plot in notebook\\n",
                "                plt.close(fig) # Close the figure to free memory\\n",
                "\\n",
                "    def save_model(self, base_filename=\\\"schrodinger_fcnn_model\\\", suffix=\\\"_final\\\"):\\n",
                "        \\\"\\\"\\\"Saves the model's state dictionary.\\\"\\\"\\\"\\n",
                "        try:\\n",
                "            filename = os.path.join(self.output_path_prefix, f\\\"{base_filename}{suffix}.pth\\\") # Use prefix\\n",
                "            torch.save(self.model.state_dict(), filename)\\n",
                "            print(f\\\"Model state dictionary saved successfully to {filename}\\\")\\n",
                "        except Exception as e:\\n",
                "            print(f\\\"Error saving model state: {e}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\\n",
    "# Adjust these parameters as needed\\n",
    "\\n",
    "# Data Parameters (Match your data file and desired subset)\\n",
    "PARAMS = {\\n",
    "    \\\"Rx\\\": 3,\\n",
    "    \\\"Nx\\\": 128,\\n",
    "    \\\"all_data\\\": 8000, # How many total samples to load and split\\n",
    "    \\\"num_data_in_filename\\\": 25000, # Value used in the data filename string\\n",
    "    \\\"data_path_prefix\\\": '../data', # Path to data directory (relative to notebook)\\n",
    "    \\\"output_path_prefix\\\": '../outputs', # Path to output directory (relative to notebook)\\n",
    "}\\n",
    "\\n",
    "# Model Hyperparameters\\n",
    "MODEL_PARAMS = {\\n",
    "    \\\"hidden_sizes\\\": [256, 128], # Example: two hidden layers\\n",
    "}\\n",
    "\\n",
    "# Training Hyperparameters\\n",
    "TRAIN_PARAMS = {\\n",
    "    \\\"batch_size\\\": 200,\\n",
    "    \\\"learning_rate\\\": 0.005, # Adjusted learning rate\\n",
    "    \\\"epochnum\\\": 500,       # Increased epochs\\n",
    "    \\\"weight_decay\\\": 1e-5,   # Adjusted weight decay\\n",
    "    \\\"shuffle_each_epoch\\\": True,\\n",
    "    \\\"early_stopping_patience\\\": 20 # Increased patience\\n",
    "}\\n",
    "\\n",
    "# --- Execution ---\\n",
    "\\n",
    "# Create the trainer instance with combined parameters\\n",
    "trainer = SchrodingerFCNNTrainer(\\n",
    "    **PARAMS,           # Unpack data parameters\\n",
    "    **MODEL_PARAMS,     # Unpack model parameters\\n",
    "    **TRAIN_PARAMS      # Unpack training parameters\\n",
    ")\\n",
    "\\n",
    "# Start the training process\\n",
    "trainer.train()\\n",
    "\\n",
    "print(\\\"\\\\n--- Notebook Execution Complete ---\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
